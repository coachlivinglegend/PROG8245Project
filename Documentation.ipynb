{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# importing the needed libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re \n",
    "import string\n",
    "import preprocessor as p\n",
    "import emoji\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the data collection process, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"./data/texts.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: talk about the nature of the data here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides information on the sentiment annotation process using the `twitter-roberta-base-sentiment model` from Hugging Face. This process is crucial for labeling the data sentiment, preparing it for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data from a CSV file, handling UTF-8 encoding issues\n",
    "df = pd.read_csv(\"./data/1000texts.csv\", encoding='utf-8-sig')\n",
    "\n",
    "# Display the first three rows of the dataframe to inspect the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the dataset, ensuring that UTF-8 encoding is used to handle any special characters in the text. \n",
    "\n",
    "The initial peek at the data with `df.head(3)` helps to confirm the structure and data types we are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert the 'Content' column into a list of sentences\n",
    "sentences = df['Content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we clean the data by removing rows with missing values to maintain the quality and consistency of our dataset. \n",
    "\n",
    "We extract the tweet content into a list to facilitate the subsequent batch processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our annotation, we will be using [`twitter-roberta-base-sentiment`](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) model on huggingface.\n",
    "\n",
    "This model is a RoBERTa-based neural network trained on approximately 58 million tweets and fine-tuned for sentiment analysis, making it highly adept at understanding the nuances of language used in tweets.\n",
    "\n",
    "**Labels Explained**\n",
    "- 0: Negative\n",
    "- 1: Neutral\n",
    "- 2: Positive\n",
    "\n",
    "These labels correspond to the sentiment expressed in each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The api requires us to group the sentences in 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group sentences into sub-lists of 10 for batch processing\n",
    "grouped_list = [sentences[n:n+10] for n in range(0, len(sentences), 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up API for Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API token and endpoint for the annotation Hugging Face's model\n",
    "API_TOKEN = \"###\"  # actual API token goes here\n",
    "API_URL = \"https://api-inference.huggingface.co/models/cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}  # Authorization header for the API request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We configure the API with the required endpoint and authentication details. We use the API_TOKEN gotten from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Processing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group sentences into sub-lists of 10 for batch processing\n",
    "grouped_list = [sentences[n:n+10] for n in range(0, len(sentences), 10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are grouped in batches of ten to optimize the API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to send data to the sentiment analysis API and get the response\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store outputs\n",
    "output = []\n",
    "\n",
    "# Loop through each group of sentences and perform sentiment analysis\n",
    "for i in range(len(grouped_list)):\n",
    "    output.append(query(grouped_list[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to send each batch to the API and store the responses. Each response includes sentiment scores and labels for the batch of tweets processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the API provides a score for each sentiment category per tweet, indicating the confidence level of each sentiment prediction. This allows us to determine the most likely sentiment expressed in each tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Extraction and Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the highest sentiment labels\n",
    "highest_labels = []\n",
    "\n",
    "# Extract the highest sentiment label from each result\n",
    "for group in output:\n",
    "    for result in group:\n",
    "        highest = max(result, key=lambda x: x['score'])\n",
    "        highest_labels.append(highest['label'].split('_')[1])\n",
    "\n",
    "# Add the highest sentiment labels back to the dataframe\n",
    "df['label'] = highest_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing, we extract the highest scoring label for each tweet and add this label back into our DataFrame. This step converts the raw output into a practical annotation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final dataframe to be saved\n",
    "df_final = df[['Content', 'label']]\n",
    "\n",
    "# Define the file path for the new CSV\n",
    "file_path = os.path.join('data', 'labeled_texts_1000.csv')\n",
    "\n",
    "# Save the dataframe to a CSV file, without the index, and handle UTF-8 encoding\n",
    "df_final.to_csv(file_path, index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully annotated dataset is saved as a CSV file, preserving the original text alongside the newly assigned sentiment labels. This file can now be used for further analysis and training predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Barbieri, F., Camacho-Collados, J., Espinosa Anke, L., & Neves, L. (2020). TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification. In Findings of the Association for Computational Linguistics: EMNLP 2020 (pp. 1644–1650). Association for Computational Linguistics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## language detection\n",
    "#def detect_lang(text):\n",
    "#    try:\n",
    "#        return detect(text)\n",
    "#    except LangDetectException:\n",
    "#        return None\n",
    "#\n",
    "#df_texts_orig = pd.read_csv('data/labeled_texts_1000.csv', encoding='utf-8-sig')\n",
    "#df_texts_orig.dropna(inplace=True)\n",
    "#\n",
    "## detect language and add a new column\n",
    "#df_texts_orig['lang'] = df_texts_orig['Content'].apply(detect_lang)\n",
    "#\n",
    "## select only English texts\n",
    "#df_eng = df_texts_orig[df_texts_orig['lang'] == 'en'].reset_index(drop=True)\n",
    "#\n",
    "#df_eng.to_csv('data/labeled_texts_eng.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_loaded = pd.read_csv('data/labeled_texts_eng.csv')\n",
    "\n",
    "df_labels = df_eng_loaded['label']\n",
    "df_labels.to_pickle('data/labels.pkl')\n",
    "\n",
    "df_texts = df_eng_loaded['Content']\n",
    "\n",
    "texts = [text for text in df_texts]\n",
    "print(texts)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoji and emoticons detection package for Python\n",
    "!pip install emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emoji package for Python\n",
    "!pip install emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweet preprocessing package for Python\n",
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace keywords in sentences\n",
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for BERT tokenizer\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed libraries\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import re \n",
    "import string\n",
    "import preprocessor as p\n",
    "import emoji\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_tag(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons(text):\n",
    "    ## formatting\n",
    "    all_emoji_emoticons = {**EMOTICONS_EMO}\n",
    "    all_emoji_emoticons = {k:v.replace(\":\",\"\").replace(\"_\",\" \").strip() for k,v in all_emoji_emoticons.items()}\n",
    "\n",
    "    kp_all_emoji_emoticons = KeywordProcessor()\n",
    "    for k,v in all_emoji_emoticons.items():\n",
    "        kp_all_emoji_emoticons.add_keyword(k, v)\n",
    "    output = kp_all_emoji_emoticons.replace_keywords(text)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    # handle abbreviations\n",
    "    normalized_text = re.sub(r'\\bfav\\b', \"favorite\", text)\n",
    "    normalized_text = re.sub(r'\\btkt\\b', \"ticket\", normalized_text)\n",
    "    normalized_text = re.sub(r'\\(gm\\)', 'good morning', normalized_text)\n",
    "    \n",
    "    # remove unnecessary information\n",
    "    normalized_text = re.sub(r'\\([^)]*(via|h/t)[^)]*\\)', '', normalized_text)\n",
    "\n",
    "    # reduce repeated characters\n",
    "    normalized_text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1\\1', normalized_text)\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_characters(text):\n",
    "    # Replacing special characters with space\n",
    "    sentence_cleaned = re.sub(r'[-_:]', ' ', text)\n",
    "\n",
    "    # Removing any character which is not a space, letter or a number\n",
    "    regular_expression_num_letters = r\"[^a-zA-Z0-9 ']\"\n",
    "    sentence_cleaned = re.sub(regular_expression_num_letters, '', sentence_cleaned)\n",
    "\n",
    "    # Removing any extra spaces\n",
    "    sentence_cleaned = re.sub(r'\\s+', ' ', sentence_cleaned)\n",
    "    \n",
    "    return sentence_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, mode='default'):\n",
    "    # remove URLs, mentions, reserved words (RT, FAV)\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    cleaned_tweet = p.clean(tweet)\n",
    "\n",
    "    # remove emojis\n",
    "    cleaned_tweet = emoji.demojize(cleaned_tweet)\n",
    "    \n",
    "    # convert emoticons to words\n",
    "    cleaned_tweet = convert_emoticons(cleaned_tweet)\n",
    "\n",
    "    # handle abbreviations\n",
    "    normalized_text = normalize_text(cleaned_tweet.lower())\n",
    "\n",
    "    # BERT Tokenizer\n",
    "    if mode == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        encoded_input = tokenizer.encode(normalized_text, add_special_tokens=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoded_input)\n",
    "        return tokens\n",
    "\n",
    "    # clean characters\n",
    "    sentence_cleaned = clean_characters(normalized_text)\n",
    "\n",
    "    # Tokenize the tweet\n",
    "    tokens = word_tokenize(sentence_cleaned)\n",
    "    \n",
    "    # POS tagging\n",
    "    pos_tagged = pos_tag(tokens)\n",
    "\n",
    "    # Remove Stopwords\n",
    "    stop_words_removed = [word for word in pos_tagged if word[0] not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = [word if get_wordnet_tag(tag) is None else lemmatizer.lemmatize(word, get_wordnet_tag(tag)) for word, tag in stop_words_removed]\n",
    "    \n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tweets(tweets, mode='default'):\n",
    "    if mode == 'bert':\n",
    "        result = []\n",
    "        for tweet in tweets:\n",
    "            tokens = clean_tweet(tweet, mode)\n",
    "            if len(tokens) <= 512:\n",
    "                result.append(tokens)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return [clean_tweet(tweet, mode) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_cleaned_tweets(texts)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bert = get_cleaned_tweets(texts, mode='bert')\n",
    "result_bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of tokens to a file using Pickle\n",
    "with open('data/preprocessing_output.pkl', 'wb') as file:\n",
    "    pickle.dump(result, file)\n",
    "\n",
    "# save dataframe as a CSV file\n",
    "preprocessed_data = [' '.join(document) for document in result]\n",
    "df_data = pd.DataFrame(preprocessed_data, columns=['Content'])\n",
    "df_data['Label'] = df_labels\n",
    "df_data.to_csv('data/preprocessed_data.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Output for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the list of tokens to a file using Pickle\n",
    "with open('data/preprocessing_output_bert.pkl', 'wb') as file:\n",
    "    pickle.dump(result_bert, file)\n",
    "\n",
    "# save dataframe as a CSV file\n",
    "preprocessed_data_bert = [' '.join(document) for document in result_bert]\n",
    "df_data = pd.DataFrame(preprocessed_data_bert, columns=['Content'])\n",
    "df_data['Label'] = df_labels\n",
    "df_data.to_csv('data/preprocessed_data_bert.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the pkl file:\n",
    "with open('data/preprocessing_output.pkl', 'rb') as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "    \n",
    "# Print the dictionary\n",
    "print(loaded_list)\n",
    "print(len(loaded_list))\n",
    "\n",
    "# print the labels\n",
    "df_labels_loaded = pd.read_pickle('data/labels.pkl')\n",
    "print(df_labels_loaded)\n",
    "print(len(df_labels_loaded))\n",
    "\n",
    "# read the CSV file\n",
    "df_data_loaded = pd.read_csv('data/preprocessed_data.csv', encoding='utf-8-sig')\n",
    "print(df_data_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file\n",
    "df_data_loaded_bert = pd.read_csv('data/preprocessed_data_bert.csv', encoding='utf-8-sig')\n",
    "print(df_data_loaded_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# to read the pkl file:\n",
    "with open('data/preprocessing_output.pkl', 'rb') as file:\n",
    "    loaded_list = pickle.load(file)\n",
    "    \n",
    "# Print the dictionary\n",
    "print(loaded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_pickle('data/labels.pkl')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(loaded_list))\n",
    "documents = [\" \".join(doc) for doc in loaded_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = vectorizer.fit_transform(X_train)\n",
    "tfidf_matrix_test = vectorizer.transform(X_test)\n",
    "print(tfidf_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')\n",
    "model.fit(tfidf_matrix_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(tfidf_matrix_test)\n",
    "\n",
    "accuracy = (y_pred == y_test).mean()\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.read_csv('data/preprocessed_data.csv')\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_sentences, test_size=0.2, random_state=42)\n",
    "train['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [[word for word in sentence.split()] for sentence in train['Content']]\n",
    "test_tokenized = [[word for word in sentence.split()] for sentence in test['Content']]\n",
    "train_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec model (skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipGram = Word2Vec(sentences=train_tokenized, vector_size=100, window=5,  min_count=1, workers=4, sg=1)\n",
    "# save the trained model\n",
    "model_skipGram.save('model/modelSkipGram.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "model_sg = Word2Vec.load('model/modelSkipGram.bin')\n",
    "print(type(model_sg.wv['binance']))\n",
    "model_sg.wv['binance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vector(model, tokenized_sentence):\n",
    "    vector = []\n",
    "    for word in tokenized_sentence:\n",
    "        if word in model.wv:\n",
    "            vector.append(model.wv[word])\n",
    "    \n",
    "    # return the average of the vectors\n",
    "    output = np.mean(vector, axis=0) if vector else np.zeros(model.vector_size)\n",
    "    \n",
    "    return output\n",
    "\n",
    "train_vectors = np.array([convert_to_vector(model_sg, sentence) for sentence in train_tokenized])\n",
    "test_vectors = np.array([convert_to_vector(model_sg, sentence) for sentence in test_tokenized])\n",
    "train_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "cross_val_score(clf, train_vectors, train['Label'], cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the classifier and get the performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train the model\n",
    "clf.fit(train_vectors, train['Label'])\n",
    "\n",
    "# predict the test set\n",
    "y_pred = clf.predict(test_vectors)\n",
    "accuracy_score(test['Label'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Embeddings Using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This documentation provides a detailed guide to preprocessing text data and extracting contextual embeddings using the BERT model. This process enhances the representation of text for advanced NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file and drop any rows with missing values immediately\n",
    "df = pd.read_csv('../data/labeled_texts_1000.csv')\n",
    "df.dropna(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract content and labels into separate variables\n",
    "X = df['Content']\n",
    "y = df['label']\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of the tweets and their labels are separated to facilitate preprocessing and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text For BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert emoticons to words\n",
    "def convert_emoticons(text):\n",
    "    # Merge emoji and emoticon dictionaries into one dictionary\n",
    "    all_emoji_emoticons = {**EMOTICONS_EMO}\n",
    "    \n",
    "    # Replace colons and underscores in keys with spaces, and trim spaces\n",
    "    all_emoji_emoticons = {k:v.replace(\":\",\"\").replace(\"_\",\" \").strip() for k,v in all_emoji_emoticons.items()}\n",
    "    \n",
    "    # Initialize a KeywordProcessor for replacing keywords\n",
    "    kp_all_emoji_emoticons = KeywordProcessor()\n",
    "    \n",
    "    # Add each emoticon and its corresponding word to the KeywordProcessor\n",
    "    for k, v in all_emoji_emoticons.items():\n",
    "        kp_all_emoji_emoticons.add_keyword(k, v)\n",
    "    \n",
    "    # Replace all emoticons in the text with corresponding words\n",
    "    return kp_all_emoji_emoticons.replace_keywords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle abbreviations and normalize text\n",
    "def normalize_text(text):\n",
    "    # Replace 'fav' with 'favorite'\n",
    "    text = re.sub(r'\\bfav\\b', \"favorite\", text)\n",
    "    \n",
    "    # Replace 'tkt' with 'ticket'\n",
    "    text = re.sub(r'\\btkt\\b', \"ticket\", text)\n",
    "    \n",
    "    # Replace '(gm)' with 'good morning'\n",
    "    text = re.sub(r'\\(gm\\)', 'good morning', text)\n",
    "    \n",
    "    # Replace '(r.i.p)' with 'rest in peace'\n",
    "    text = re.sub(r'\\(r.i.p\\)', 'rest in peace', text)\n",
    "    \n",
    "    # Remove parenthetical references (typically credits like via or hat tips)\n",
    "    text = re.sub(r'\\([^)]*(via|h/t)[^)]*\\)', '', text)\n",
    "    \n",
    "    # Reduce excess letter repetitions (more than two) to two\n",
    "    text = re.sub(r'([a-zA-Z])\\1{2,}', r'\\1\\1', text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_characters(text):\n",
    "    # Replace special characters (hyphens, underscores, colons) with a space\n",
    "    text = re.sub(r'[-_:]', ' ', text)\n",
    "    \n",
    "    # Normalize line endings, replacing carriage return and newline with just newline\n",
    "    normalized_text = re.sub(r'\\r\\n', '\\n', text)\n",
    "    \n",
    "    # Remove decimal points used in numbers\n",
    "    no_decimal_text = re.sub(r'(\\d)\\.(\\d)', r'\\1\\2', normalized_text)\n",
    "    \n",
    "    # Remove characters that are not letters, numbers, basic punctuation, or newline\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9 '.\\n]\", '', no_decimal_text)\n",
    "    \n",
    "    # Reduce multiple consecutive dots to a single dot\n",
    "    cleaned_text = re.sub(r'\\.{2,}', '.', cleaned_text)\n",
    "    \n",
    "    # Reduce multiple consecutive question marks to a single one\n",
    "    cleaned_text = re.sub(r'\\?{2,}', '?', cleaned_text)\n",
    "    \n",
    "    # Replace multiple consecutive newlines with a single period or space\n",
    "    cleaned_text = re.sub(r'(\\n)+', lambda m: '.' if m.group().startswith('\\n') and not m.group().endswith('.') else '. ', cleaned_text)\n",
    "    \n",
    "    # Clean up multiple spaces or periods into a single space or period\n",
    "    cleaned_text = re.sub(r'\\. \\.', '. ', cleaned_text)\n",
    "    \n",
    "    # Reduce multiple spaces to a single space\n",
    "    cleaned_text = re.sub(r' {2,}', ' ', cleaned_text)\n",
    "    \n",
    "    # Return the cleaned text, stripped of leading/trailing whitespace\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    # Configure the preprocessor to remove URLs, mentions, and reserved words like RT or FAV\n",
    "    p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.RESERVED)\n",
    "    \n",
    "    # Clean the tweet using preprocessor settings\n",
    "    cleaned_tweet = p.clean(tweet)\n",
    "    \n",
    "    # Convert all emojis in the tweet to text\n",
    "    cleaned_tweet = emoji.demojize(cleaned_tweet)\n",
    "    \n",
    "    # Convert emoticons within the tweet to words\n",
    "    cleaned_tweet = convert_emoticons(cleaned_tweet)\n",
    "    \n",
    "    # Normalize text to handle abbreviations and remove unnecessary parts\n",
    "    normalized_text = normalize_text(cleaned_tweet.lower())\n",
    "    \n",
    "    # Clean characters and correct formatting issues\n",
    "    sentence_cleaned = clean_characters(normalized_text)\n",
    "    \n",
    "    # Return the fully cleaned and processed tweet\n",
    "    return sentence_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_tweets(tweets):\n",
    "    # Process a list of tweets, cleaning each one using clean_tweet function\n",
    "    return [clean_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning process to all tweets in X and store results\n",
    "result = get_cleaned_tweets(X)\n",
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add BERT Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_special_tokens(sentence):\n",
    "    # Step 1: Adding the [CLS] token at the beginning\n",
    "    sentence_with_cls = \"[CLS] \" + sentence\n",
    "    \n",
    "    # Step 2: Adding the [SEP] token before each full stop\n",
    "    split_sentence = sentence_with_cls.split('.')\n",
    "    sentence_with_sep = \" [SEP].\".join(split_sentence)\n",
    "    \n",
    "    # Clean up to handle cases where [SEP] might be added at the end unnecessarily\n",
    "    sentence_with_sep = sentence_with_sep.replace(\" [SEP].\", \" [SEP]\").rstrip()\n",
    "    \n",
    "    return sentence_with_sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT requires specific tokens to be added to the text. This function inserts the [CLS] token at the start and the [SEP] token at sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to all sentences in the results list\n",
    "processed_results = [add_special_tokens(sentence) for sentence in result]\n",
    "processed_results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize all the processed results\n",
    "tokenized_results = [tokenizer.tokenize(sentence) for sentence in processed_results]\n",
    "tokenized_results\n",
    "\n",
    "# Convert tokens to their respective IDs in the BERT vocabulary\n",
    "indexed_tokens_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_results]\n",
    "indexed_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the preprocessed text and convert the tokens into indices that correspond to BERT's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Segment IDs and Attention Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for segment IDs and attention masks\n",
    "token_type_list = []\n",
    "attention_mask_list = []\n",
    "\n",
    "# Generate segment IDs and attention masks for each sentence\n",
    "for indexed_sentence in indexed_tokens_list:\n",
    "    # For each sentence, all tokens belong to the same segment, so use 0\n",
    "    segment_ids = [0] * len(indexed_sentence)\n",
    "    token_type_list.append(segment_ids)\n",
    "    \n",
    "    # If you're not padding, all tokens are real, so the attention mask is all 1s\n",
    "    attention_mask = [1] * len(indexed_sentence)\n",
    "    attention_mask_list.append(attention_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment IDs indicate to the model different segments of the input, while attention masks allow the model to ignore padding during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize padded lists\n",
    "padded_input_ids = []\n",
    "padded_attention_mask = []\n",
    "\n",
    "# Find the maximum sequence length in your batch\n",
    "max_length = max(len(tokens) for tokens in indexed_tokens_list)\n",
    "\n",
    "for tokens in indexed_tokens_list:\n",
    "    # Calculate the number of padding tokens needed\n",
    "    num_padding_tokens = max_length - len(tokens)\n",
    "    \n",
    "    # Pad the input IDs with zeros (assuming 0 is your padding token)\n",
    "    padded_tokens = tokens + [0] * num_padding_tokens\n",
    "    padded_input_ids.append(padded_tokens)\n",
    "    \n",
    "    # Pad the attention mask where actual tokens are marked with 1 and padding tokens with 0\n",
    "    padded_mask = [1] * len(tokens) + [0] * num_padding_tokens\n",
    "    padded_attention_mask.append(padded_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniform input length is crucial for batch processing in neural networks. This step pads shorter sequences with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section describes loading the pre-trained BERT model, converting data into tensors, and running the model to extract contextual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Convert the lists of integers into tensors\n",
    "input_ids_tensor = torch.tensor(padded_input_ids)\n",
    "attention_mask_tensor = torch.tensor(padded_attention_mask)\n",
    "\n",
    "# Run the model and get the outputs\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor, attention_mask=attention_mask_tensor)\n",
    "    hidden_states = outputs[2]  # Hidden states from all BERT layers\n",
    "    word_embeddings = outputs.last_hidden_state  # The last layer's output\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the embeddings from a file\n",
    "word_embeddings = np.load('./data/word_embeddings.npy')\n",
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv file\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./data/labeled_texts_1000.csv')\n",
    "df.dropna(inplace=True)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_embeddings, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the word embeddings for each text sample\n",
    "X_avg = word_embeddings.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, you can split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_avg, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import svm\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "conf_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=clf.classes_)\n",
    "conf_matrix_display.plot()\n",
    "# Detailed classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameter Tuning and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'linear']}\n",
    "\n",
    "# Grid search with 10-fold cross-validation\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=5)  # Set cv=10 for 10-fold CV\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the confusion matrix\n",
    "y_pred = grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "conf_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=clf.classes_)\n",
    "conf_matrix_display.plot()\n",
    "# Detailed classification report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming 'y' contains integer labels for categories\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_embeddings, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow import keras\n",
    "\n",
    "num_classes = y_train_encoded.shape[1]  # Number of unique classes\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(100, input_shape=(word_embeddings.shape[1], word_embeddings.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')  # Output layer for multi-class classification\n",
    "])\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"./models/best-lstm.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',  # Loss function for multi-class classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_encoded,  # Use one-hot encoded labels\n",
    "                    epochs=4,  # May need adjustment\n",
    "                    batch_size=32,  # May need adjustment\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks\n",
    "                    )  # Fraction of data to use as validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training and validation accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "best_model = load_model('./models/best-lstm.keras')\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test_encoded)\n",
    "print('Test Accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(units=hp.Int('units', min_value=50, max_value=200, step=50), return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Tuning the learning rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')\n",
    "\n",
    "tuner.search(X_train, y_train_encoded, epochs=50, validation_split=0.2)\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best model\n",
    "best_tuner_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_tuner_model.summary()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = best_tuner_model.predict(X_test)\n",
    "\n",
    "# Since we're doing multi-class classification, 'y_pred' will contain probabilities for each class\n",
    "# To convert these probabilities into class labels, you can use 'argmax' which returns the index of the maximum value\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_pred_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'y_test' contains the actual labels\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred_labels))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "conf_matrix_display = ConfusionMatrixDisplay(conf_matrix, display_labels=clf.classes_)\n",
    "conf_matrix_display.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment and Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install customtkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import customtkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def button_callback():\n",
    "    print(\"button clicked\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = customtkinter.CTk()\n",
    "app.title(\"Custom Tkinter\")\n",
    "app.geometry(\"820x480\")\n",
    "\n",
    "app.grid_columnconfigure((0), weight=1)\n",
    "# app.grid_columnconfigure((1), weight=1)\n",
    "app.grid_columnconfigure((2), weight=1)\n",
    "app.grid_columnconfigure((3), weight=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbox = customtkinter.CTkTextbox(app, wrap=\"word\", corner_radius=15)\n",
    "\n",
    "textbox.insert(\"0.0\", \"What is on you mind?\")  # insert at line 0 character 0\n",
    "\n",
    "text = textbox.get(\"0.0\", \"end\")  # get text from line 0 character 0 till the end\n",
    "\n",
    "textbox.grid(row=0, column=0, columnspan=4, padx=50, sticky=\"nsew\", pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button = customtkinter.CTkButton(app, text=\"PREDICT\", width=200, height=30, command=button_callback, fg_color=\"#f40e7d\", hover_color=\"#d4116f\", corner_radius=15, font=(\"Arial\", 12))\n",
    "button.grid(row=1, column=2, pady=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = customtkinter.CTkTextbox(app, width=400, height=100, wrap=\"word\", corner_radius=15)\n",
    "\n",
    "sentiment.insert(\"0.0\", \"The emotion here is giving...\")  # insert at line 0 character 0\n",
    "text_sentiment = sentiment.get(\"0.0\", \"end\")  # get text from line 0 character 0 till the end\n",
    "# textbox.delete(\"0.0\", \"end\")  # delete all text\n",
    "sentiment.configure(state=\"disabled\")  # configure textbox to be read-only\n",
    "\n",
    "sentiment.grid(row=3, column=2, pady=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf4gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
